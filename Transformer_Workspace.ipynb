{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = {2017: 0, 2018: 1}\n",
    "month = {9: 0, 10: 1, 11: 2, 12: 3, 1: 4, 2: 5}\n",
    "day = {0: 1, 1: 2, 2: 3, 3: 4, 4: 5}\n",
    "time = {9: 0, 10: 1, 11:2, 12: 3, 13: 4, 14: 5, 15: 6}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, d_inputs, d_outs, num_layers, d_model, nhead, d_feed, dropout, activation=\"relu\"):\n",
    "        encoder = nn.TransformerEncoderLayer(d_inputs, nhead, d_feed, dropout, activation)\n",
    "        self.embed = TimeEmbedding(d_embed)\n",
    "        self.transformer = nn.TransformerEncoder(encoder, num_layers)\n",
    "        self.decoder = nn.Linear(d_inputs, d_outs)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        inputs = self.embed(inputs)\n",
    "        outputs = self.transformer(inputs)\n",
    "        outputs = self.decoder(outputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeEmbedding(nn.Module):\n",
    "    def __init__(self, d_embed):\n",
    "        super(TimeEmbedding, self).__init__()\n",
    "        self.d_embed = d_embed\n",
    "        self.year_embed = nn.Embedding(2, d_embed)\n",
    "        self.month_embed = nn.Embedding(6, d_embed)\n",
    "        self.day_of_week_embed = nn.Embedding(5, d_embed)\n",
    "        self.hour_embed = nn.Embedding(7, d_embed)\n",
    "        \n",
    "        ## それか、これらのfeatureをnn.Linearに通してd_modelを小さくするか\n",
    "        ## concat にするか sum　にするか\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        init_range = 0.1\n",
    "        self.year_embed.weight.data.uniform_(-init_range, init_range)\n",
    "        self.month_embed.weight.data.uniform_(-init_range, init_range)\n",
    "        self.day_of_week_embed.weight.data.uniform_(-init_range, init_range)\n",
    "        self.hour_embed.weight.data.uniform_(-init_range, init_range)\n",
    "        \n",
    "    def forward(self, inputs, concat=False): # inputs -> (batch, seq, input_d)\n",
    "        inputs = torch.LongTensor(inputs)\n",
    "        year_embed = self.month_embed(inputs[:, :, 4]) # (batch, seq, d_model)\n",
    "        month_embed = self.month_embed(inputs[:, :, 5])\n",
    "        day_of_week_embed = self.day_of_week_embeb(inputs[:, :, 7])\n",
    "        hour_embed = self.hour_embed(inputs[:, :, 6])\n",
    "            \n",
    "        if arg == \"concat\":\n",
    "\n",
    "            time_embed = torch.cat((year_embed, month_embed, day_of_week_embed, hour_embed), axis=2)\n",
    "            inputs_embed = torch.cat((inputs[:, :, 0:4], time_embed), axis=2)\n",
    "\n",
    "            return inputs_embed\n",
    "\n",
    "        return inputs[:, :, 0:4] + year_embed + month_embed + day_of_week_embed + hour_embed\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
